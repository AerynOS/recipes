From 5c9ed91c422261d2d51b24547d8f1e26d30294a2 Mon Sep 17 00:00:00 2001
From: Jonathan Toledo <overjt@users.noreply.github.com>
Date: Wed, 30 Oct 2024 10:47:28 -0500
Subject: [PATCH 1/3] Closes: 19963

Add support for new models o1-preview, o1-Mini, and Claude 3.5 Sonnet
---
 crates/copilot/src/copilot_chat.rs | 18 ++++++++++++++++++
 1 file changed, 18 insertions(+)

diff --git a/crates/copilot/src/copilot_chat.rs b/crates/copilot/src/copilot_chat.rs
index c5ba1bfc6a589..69050a8d3b6d8 100644
--- a/crates/copilot/src/copilot_chat.rs
+++ b/crates/copilot/src/copilot_chat.rs
@@ -35,6 +35,12 @@ pub enum Model {
     Gpt4,
     #[serde(alias = "gpt-3.5-turbo", rename = "gpt-3.5-turbo")]
     Gpt3_5Turbo,
+    #[serde(alias = "o1-preview", rename = "o1-preview-2024-09-12")]
+    O1Preview,
+    #[serde(alias = "o1-mini", rename = "o1-mini-2024-09-12")]
+    O1Mini,
+    #[serde(alias = "claude-3-5-sonnet", rename = "claude-3.5-sonnet")]
+    Claude3_5Sonnet,
 }
 
 impl Model {
@@ -43,6 +49,9 @@ impl Model {
             "gpt-4o" => Ok(Self::Gpt4o),
             "gpt-4" => Ok(Self::Gpt4),
             "gpt-3.5-turbo" => Ok(Self::Gpt3_5Turbo),
+            "o1-preview" => Ok(Self::O1Preview),
+            "o1-mini" => Ok(Self::O1Mini),
+            "claude-3-5-sonnet" => Ok(Self::Claude3_5Sonnet),
             _ => Err(anyhow!("Invalid model id: {}", id)),
         }
     }
@@ -52,6 +61,9 @@ impl Model {
             Self::Gpt3_5Turbo => "gpt-3.5-turbo",
             Self::Gpt4 => "gpt-4",
             Self::Gpt4o => "gpt-4o",
+            Self::O1Mini => "o1-mini",
+            Self::O1Preview => "o1-preview",
+            Self::Claude3_5Sonnet => "claude-3-5-sonnet",
         }
     }
 
@@ -60,6 +72,9 @@ impl Model {
             Self::Gpt3_5Turbo => "GPT-3.5",
             Self::Gpt4 => "GPT-4",
             Self::Gpt4o => "GPT-4o",
+            Self::O1Mini => "o1-mini",
+            Self::O1Preview => "o1-preview",
+            Self::Claude3_5Sonnet => "Claude 3.5 Sonnet",
         }
     }
 
@@ -68,6 +83,9 @@ impl Model {
             Self::Gpt4o => 128000,
             Self::Gpt4 => 8192,
             Self::Gpt3_5Turbo => 16385,
+            Self::O1Mini => 128000,
+            Self::O1Preview => 128000,
+            Self::Claude3_5Sonnet => 200_000,
         }
     }
 }

From 750af2c5247b27efb89b585c3a3684ce6dcc1ff5 Mon Sep 17 00:00:00 2001
From: Jonathan Toledo <overjt@users.noreply.github.com>
Date: Wed, 30 Oct 2024 11:29:06 -0500
Subject: [PATCH 2/3] Copilot: Claude 3.5 Sonnet count token

---
 .../src/provider/copilot_chat.rs              | 22 +++++++++++++------
 1 file changed, 15 insertions(+), 7 deletions(-)

diff --git a/crates/language_model/src/provider/copilot_chat.rs b/crates/language_model/src/provider/copilot_chat.rs
index 58b486921ab81..3e3a6d1768f4e 100644
--- a/crates/language_model/src/provider/copilot_chat.rs
+++ b/crates/language_model/src/provider/copilot_chat.rs
@@ -30,6 +30,7 @@ use crate::{
 };
 use crate::{LanguageModelCompletionEvent, LanguageModelProviderState};
 
+use super::anthropic::count_anthropic_tokens;
 use super::open_ai::count_open_ai_tokens;
 
 const PROVIDER_ID: &str = "copilot_chat";
@@ -179,13 +180,20 @@ impl LanguageModel for CopilotChatLanguageModel {
         request: LanguageModelRequest,
         cx: &AppContext,
     ) -> BoxFuture<'static, Result<usize>> {
-        let model = match self.model {
-            CopilotChatModel::Gpt4o => open_ai::Model::FourOmni,
-            CopilotChatModel::Gpt4 => open_ai::Model::Four,
-            CopilotChatModel::Gpt3_5Turbo => open_ai::Model::ThreePointFiveTurbo,
-        };
-
-        count_open_ai_tokens(request, model, cx)
+        match self.model {
+            CopilotChatModel::Claude3_5Sonnet => count_anthropic_tokens(request, cx),
+            _ => {
+                let model = match self.model {
+                    CopilotChatModel::Gpt4o => open_ai::Model::FourOmni,
+                    CopilotChatModel::Gpt4 => open_ai::Model::Four,
+                    CopilotChatModel::Gpt3_5Turbo => open_ai::Model::ThreePointFiveTurbo,
+                    CopilotChatModel::O1Preview => open_ai::Model::Four,
+                    CopilotChatModel::O1Mini => open_ai::Model::Four,
+                    CopilotChatModel::Claude3_5Sonnet => unreachable!(),
+                };
+                count_open_ai_tokens(request, model, cx)
+            }
+        }
     }
 
     fn stream_completion(

From 4e03c3cfac8dd6ef37f2ec56ce0aeefcf3b76520 Mon Sep 17 00:00:00 2001
From: Jonathan Toledo <overjt@users.noreply.github.com>
Date: Thu, 31 Oct 2024 08:52:06 -0500
Subject: [PATCH 3/3] o1 models support

---
 crates/copilot/src/copilot_chat.rs            | 46 ++++++++++-----
 .../src/provider/copilot_chat.rs              | 58 ++++++++++++-------
 2 files changed, 68 insertions(+), 36 deletions(-)

diff --git a/crates/copilot/src/copilot_chat.rs b/crates/copilot/src/copilot_chat.rs
index 69050a8d3b6d8..55b477fe2a4f4 100644
--- a/crates/copilot/src/copilot_chat.rs
+++ b/crates/copilot/src/copilot_chat.rs
@@ -44,6 +44,17 @@ pub enum Model {
 }
 
 impl Model {
+    pub fn uses_streaming(&self) -> bool {
+        match self {
+            Self::Gpt4o => true,
+            Self::Gpt4 => true,
+            Self::Gpt3_5Turbo => true,
+            Self::O1Mini => false,
+            Self::O1Preview => false,
+            Self::Claude3_5Sonnet => true,
+        }
+    }
+
     pub fn from_id(id: &str) -> Result<Self> {
         match id {
             "gpt-4o" => Ok(Self::Gpt4o),
@@ -105,7 +116,7 @@ impl Request {
         Self {
             intent: true,
             n: 1,
-            stream: true,
+            stream: model.uses_streaming(),
             temperature: 0.1,
             model,
             messages,
@@ -131,7 +142,8 @@ pub struct ResponseEvent {
 pub struct ResponseChoice {
     pub index: usize,
     pub finish_reason: Option<String>,
-    pub delta: ResponseDelta,
+    pub delta: Option<ResponseDelta>,
+    pub message: Option<ResponseDelta>,
 }
 
 #[derive(Debug, Deserialize)]
@@ -351,9 +363,23 @@ async fn stream_completion(
     if let Some(low_speed_timeout) = low_speed_timeout {
         request_builder = request_builder.read_timeout(low_speed_timeout);
     }
+    let is_streaming = request.stream;
+
     let request = request_builder.body(AsyncBody::from(serde_json::to_string(&request)?))?;
     let mut response = client.send(request).await?;
-    if response.status().is_success() {
+
+    if !response.status().is_success() {
+        let mut body = Vec::new();
+        response.body_mut().read_to_end(&mut body).await?;
+        let body_str = std::str::from_utf8(&body)?;
+        return Err(anyhow!(
+            "Failed to connect to API: {} {}",
+            response.status(),
+            body_str
+        ));
+    }
+
+    if is_streaming {
         let reader = BufReader::new(response.into_body());
         Ok(reader
             .lines()
@@ -385,19 +411,9 @@ async fn stream_completion(
     } else {
         let mut body = Vec::new();
         response.body_mut().read_to_end(&mut body).await?;
-
         let body_str = std::str::from_utf8(&body)?;
+        let response: ResponseEvent = serde_json::from_str(body_str)?;
 
-        match serde_json::from_str::<ResponseEvent>(body_str) {
-            Ok(_) => Err(anyhow!(
-                "Unexpected success response while expecting an error: {}",
-                body_str,
-            )),
-            Err(_) => Err(anyhow!(
-                "Failed to connect to API: {} {}",
-                response.status(),
-                body_str,
-            )),
-        }
+        Ok(futures::stream::once(async move { Ok(response) }).boxed())
     }
 }
diff --git a/crates/language_model/src/provider/copilot_chat.rs b/crates/language_model/src/provider/copilot_chat.rs
index 3e3a6d1768f4e..fdc9e917d7643 100644
--- a/crates/language_model/src/provider/copilot_chat.rs
+++ b/crates/language_model/src/provider/copilot_chat.rs
@@ -217,7 +217,8 @@ impl LanguageModel for CopilotChatLanguageModel {
             }
         }
 
-        let request = self.to_copilot_chat_request(request);
+        let copilot_request = self.to_copilot_chat_request(request);
+        let is_streaming = copilot_request.stream;
         let Ok(low_speed_timeout) = cx.update(|cx| {
             AllLanguageModelSettings::get_global(cx)
                 .copilot_chat
@@ -228,28 +229,43 @@ impl LanguageModel for CopilotChatLanguageModel {
 
         let request_limiter = self.request_limiter.clone();
         let future = cx.spawn(|cx| async move {
-            let response = CopilotChat::stream_completion(request, low_speed_timeout, cx);
-            request_limiter.stream(async move {
-                let response = response.await?;
-                let stream = response
-                    .filter_map(|response| async move {
-                        match response {
-                            Ok(result) => {
-                                let choice = result.choices.first();
-                                match choice {
-                                    Some(choice) => Some(Ok(choice.delta.content.clone().unwrap_or_default())),
-                                    None => Some(Err(anyhow::anyhow!(
-                                        "The Copilot Chat API returned a response with no choices, but hadn't finished the message yet. Please try again."
-                                    ))),
+                let response = CopilotChat::stream_completion(copilot_request, low_speed_timeout, cx);
+                request_limiter.stream(async move {
+                    let response = response.await?;
+                    let stream = response
+                        .filter_map(move |response| async move {
+                            match response {
+                                Ok(result) => {
+                                    let choice = result.choices.first();
+                                    match choice {
+                                        Some(choice) if !is_streaming => {
+                                            match &choice.message {
+                                                Some(msg) => Some(Ok(msg.content.clone().unwrap_or_default())),
+                                                None => Some(Err(anyhow::anyhow!(
+                                                    "The Copilot Chat API returned a response with no message content"
+                                                ))),
+                                            }
+                                        },
+                                        Some(choice) => {
+                                            match &choice.delta {
+                                                Some(delta) => Some(Ok(delta.content.clone().unwrap_or_default())),
+                                                None => Some(Err(anyhow::anyhow!(
+                                                    "The Copilot Chat API returned a response with no delta content"
+                                                ))),
+                                            }
+                                        },
+                                        None => Some(Err(anyhow::anyhow!(
+                                            "The Copilot Chat API returned a response with no choices, but hadn't finished the message yet. Please try again."
+                                        ))),
+                                    }
                                 }
+                                Err(err) => Some(Err(err)),
                             }
-                            Err(err) => Some(Err(err)),
-                        }
-                    })
-                    .boxed();
-                Ok(stream)
-            }).await
-        });
+                        })
+                        .boxed();
+                    Ok(stream)
+                }).await
+            });
 
         async move {
             Ok(future
